from clean_data import clean
from build_features import split_sequences, process_data
from visualize import predict_plot
from utils import moving_average

import argparse
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler

import tensorflow.keras as K
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import LSTM, GRU
from tensorflow.keras.layers import Dropout

parser = argparse.ArgumentParser()
parser.add_argument('-t', '--type', nargs='?', type=str, 
    choices=['lstm', 'gru'],
    help='Choice for RNN architecture.')
parser.add_argument('-d', '--dataset', nargs='?', type=str, 
    choices=['deep', 'shallow'],
    help='Two current dataset generated by deep and shalow LoRa sensor.')
parser.add_argument('--split_ratio', nargs='?', type=float, default=0.8, 
    help='Ratio for train-test split.')
parser.add_argument('-s', '--step', nargs='?', type=int, 
    help='Value for timestamp.')
parser.add_argument('-l', '--layers', nargs='?', type=int, default=2,
    help='Number of hidden layers.')
parser.add_argument('-n', '--neurons', nargs='?', type=int, default=2, 
    help='Number of neurons per hidden layer.')
parser.add_argument('-o', '--optimizer', nargs='?', type=str, 
    choices=['sgd', 'rmsprop', 'adam'],
    help='Algorithm for the minimization of loss function.')
parser.add_argument('-c', '--loss', nargs='?', type=str, 
    choices=['mse', 'mae'],
    help='Loss function.')
parser.add_argument('--lr', nargs='?', type=float, default=0.01,
    help='Learning rate for the optimizer.')
parser.add_argument('--epochs', nargs='+', type=int,
    help='Number of training epochs.')
parser.add_argument('--batch_size', nargs='?', type=int,
    help='Batch size.')
parser.add_argument('--save', action='store_true',
    help='Save the best model after the training is done.')
args = parser.parse_args()

def piecewise_constant_fn(epoch):
    if epoch < 25:
        return args.lr
    else:
        return args.lr * 0.1

def define_model(x_train):
    # model architecture
    model = Sequential()

    if args.rnn == 'gru':
        nn = GRU
    else:
        nn = LSTM

    if args.layers >= 2:
        seq = True
    else:
        seq = False

    model.add(nn(args.neurons, return_sequences=seq,
                  input_shape=(x_train.shape[1], x_train.shape[2])))
    model.add(Dropout(0.2))

    if args.layers >= 2:
        for layer in range(1, args.layers):
            if layer == (args.layers - 1):
                seq = False
            model.add(nn(args.neurons, activation='elu', 
                         kernel_initializer='he_normal',
                         return_sequences=seq))
            model.add(Dropout(0.2))
    model.add(Dense(1))

    # optimizer
    if args.optimizer == 'sgd':
        optim = K.optimizers.SGD(lr=args.lr, momentum=0.9, nesterov=True)
    elif args.optimizer == 'rmsprop':
        optim = K.optimizers.RMSprop(learning_rate=args.lr, rho=0.9)
    elif args.optimizer == 'adam':
        optim = K.optimizers.Adam(learning_rate=args.lr)

    # loss function
    if args.loss == 'mae':
        loss = 'mean_absolute_error'
    else:
        loss = 'mean_squared_error'

    # build
    model.compile(optimizer=optim, loss=loss, metrics=['mae'])
    return model

def train(model, save_dir, x_train, y_train, x_valid, y_valid, epoch):
    lr_scheduler = K.callbacks.LearningRateScheduler(piecewise_constant_fn)
    if args.save:
        callback = K.callbacks.ModelCheckpoint(
            save_dir, monitor='val_loss', verbose=1,
            save_best_only=True, mode='min', period=1
            )
        losses = model.fit(
            x_train, y_train, epochs=epoch, batch_size=args.batch_size, 
            verbose=1, callbacks=[callback, lr_scheduler], 
            validation_data=(x_valid, y_valid), shuffle=False
            )
    else:
        losses = model.fit(
            x_train, y_train, epochs=epoch, batch_size=args.batch_size, 
            verbose=1, callbacks=[lr_scheduler], 
            validation_data=(x_valid, y_valid), shuffle=False
            )
    return model, losses

def evaluate(model, x_valid, y_valid, x_test, y_test):
    scores = model.evaluate(
        x_test, y_test, batch_size=args.batch_size, verbose=1
        )
    valid_scores = model.evaluate(
        x_valid, y_valid, batch_size=args.batch_size, verbose=1
        )
    return scores, valid_scores

def main():
    if args.trainset == 'deep':
        data_path = 'data/deep.csv'
        test_data_path = 'data/shallow.csv'
        save_dir = '../models/deep.h5'
    elif args.trainset == 'shallow':
        data_path = 'data/shallow.csv'
        test_data_path = 'data/deep.csv'
        save_dir = '../models/shallow.h5'

    raw_data = pd.read_csv(data_path)
    raw_test_data = pd.read_csv(test_data_path)
    data = clean(raw_data, args.step, temp=False, absolute=True)
    test_data = clean(raw_test_data, args.step, temp=False, absolute=True)
    x_train, y_train,x_valid, y_valid, x_test, y_test, \
        scaler = process_data(data, test_data, args.step, args.ratio)
    print('Train set shape: ', x_train.shape, y_train.shape)
    print('Valid set shape: ', x_valid.shape, y_valid.shape)
    print('Test set shape: ', x_test.shape, y_test.shape)

    model = define_model(x_train)
    model, losses = train(
        model, save_dir, x_train, y_train, x_valid, y_valid, args.epoch
        )
    print(model.summary())

    predict_plot(model, x_valid, y_valid, x_train, y_train, scaler, losses=losses)

if __name__ == "__main__":
    main()
